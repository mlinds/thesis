\chapter{Discussion}


\section{Limitations of ICESat-2 ATL03 data}
The starting point of this method is the level 2A product ATL03 data from NASA. It consists of raw photon locations and data about the atmospheric and geophysical parameters. 

\subsection{Tidal model errors}
\subsection{Misclassified photons in ATL03 data}

This the subsurface finding algorithm depends on the accuracy of the ATL03 signal classification for ocean photons. Excluding the ocean surface signal from the KDE algorithm is crucial because the surface signal is often significantly more dense in the vertical direction than the bathymetric signal.

The default classification is often reliable, but when there is a large area where the bathymetric surface is shallow nearly parallel to the sea surface, the there can be misclassifications when unfortunately can miss large areas of very high quality data. An example of this is shown in figure \ref{fig:ageeba_bad_classes}. The actual sea surface is not classified as a high confidence ocean surface return, likely due to the strong bathymetry that is close and parallel to the sea surface. Because of this, the filtering step within the KDE signal finding algorithm cannot find an ocean surface, and therefore all the points are filtered out. This could potentially be mitigated by a different filtering strategy that assumes the sea surface is at a geoidal height of 0. This could be very feasible in microtidal areas where the tidal signal has a smaller impacts.

\pdfcomment{temp figure, replace with matplotlib figure with axis labels. Maybe also include a plan view map with scale bar to understand the geographic context}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/ageeba_beach_example.png}
    \caption{Classification of photons from 2021-07-19, Beam gt3r, reference ground track 396. The two parallel straight lines from 200 to 800 are the sea surface and the bathymetric signal. The default signal classification algorithm misclassifies the bathymetric points}
    \label{fig:ageeba_bad_classes}
\end{figure}



\subsection{Limitations in Photon Geolocation Accuracy}

The pointing determination algorithm used by the satellite has a high vertical accuracy, but there is an inherent limitation on the horizontal accuracy. The current best estimate of the vertical accuracy is 0.17cm, and the estimate of the x and y position uncertainty is 5m.

\pdfcomment{From data users guide, either cite that or better, find the published literature it comes from}.

This uncertainty, and the horizontal refraction, are more likely second order effects. Because the kriging is used to create a product of 50m resolution, any uncertainty introduced by this will be masked by the interpolation to a 50m grid.


\subsection{Errors in Refraction Correction}

The refraction correction method used accounts for the additional horizontal error that is introduced by off-pointing. However, there are several other second-order effects that are not considered by the methodology. One of these is the estimation of the refractive index; the temperature and salinity affect the speed at which the water transmits light. By assuming a default value it introduces an error on the order of X.XXm \pdfcomment{Look this up to plug in some numbers to the formula to check}. This could be corrected by either estimating in advance a value for each local site, or by connecting the algorithm to an API that can provide a temperature and salinity value. 

Another potential source of error is the slope of the water surface. Since there is a slope to the water surface, this affects the bounce angle of the photon. This can be corrected for and some papers that investigate ICESat-2 bathymetry have attempted to correct for it. For this project the magnitude was considered small enough that the effect was within the margin of error of the method.

The curvature of the earth can also affect the accuracy of the refraction correction. For longer transects, this affect can be corrected for with an additional correction suggested by \citeauthor{Parrish2019}. \pdfcomment{Just added this correction and improved error for one site by O 0.001m :|}

\subsection{Limited Spatial Coverage in Islands}

In support of the vegetation mission of ICESat-2, the instrument is sometimes pointed up to several degrees to the side of the reference ground tracks when the satellite passes over land. This increases the spatial density of points at the expense of the temporal resolution. For bathymetric purposes the increased spatial resolution gives a more even  coverage of nearshore zone bathymetry. 

However, the land mask that is used to determine the off-pointing strategy has a limited resolution, and therefore some island nations do not benefit from the increased spatial density. This was noted when trying to collect data from Fiji and the Maldives. Due to apparently being located within the off-pointing zones, both of the aforementioned islands only have tracks which are 3km apart. They can still potentially collect bathymetry data if conditions are otherwise good, but the further reduction in spatial coverage limits the accuracy of the kriging method. This is unfortunate because many of the states that are at the highest need of detailed bathymetry for numerical studies are big ocean island nations. The tradeoff for this scenario is that the temporal resolution is significantly better, so the spaceborne lidar could be useful for studying the changes over time. 

\subsection{Inherent Uncertainty of KDE Method}
There are a number of input parameters to the filtering and the density-based bathymetry finding methods. These parameters can be optimized for each site to reduce the RMSE error as much as possible if there is some validation data available. However since the end goal of the project is to be able to improve estimates without using any in situ data, ideally there would be no need for optimization based on the site.

Currently the globally-set parameters are sufficient to extract bathymetry without any tuning for all of the case studies that are investigated. However, the inability to tune in advance is a limitation. 

One possible future step would be to gather even more validation sites, and explore which other variables might influence the best parameter setting. It is possible that there are certain site variables which predict the optimal parameter options. Even so, upscaling of validation sites would allow better insight into which variables predict the presence of valid data.  

\subsection{ATL03 Data Quality Issues}\label{sec:discussion-photon-issues}

There are a number of known issues with the ICESat-2 data. They are either due to atmospheric and environmental conditions, or due to limitations of the instrument. Many of them can be detected in advance, and then the effected granule data can be thrown out or the issue otherwise corrected for. However, there might be some edge cases related to these issues that cause either false bathymetric signal points, or cause the algorithm to miss valid bathymetric data. These data issues could present an issue for the scaling up the signal finding without any manual intervention. Currently the process is run without any intervention, but the sites are small enough to manually check several of the transects.

The following known data issues could effect the results of the KDE signal finding algorithm:

\pdfcomment{need to decide which of these apply to subsurface data, some will not (i.e. )}
% \begin{itemize}
%     \item Clouds
%     \item Multiple telemetry bands
%     \item Photon noise bursts % yes
%     \item Apparent multiple surface returns % yes
%     \item Specular returns
%     \item multiple scattering % def an issue, will look at correction
%     \item TEP misidentified as signal % an issue but hard to avoid and rare (per nasa)
%     \item errors following DMUs % 
%     \item background count rate and saturation
%     \item data gaps due to TEP crossing surface % filtering mechanism will catch this
% \end{itemize}

\subsubsection{Clouds}

Clouds reflect sunlight which causes a higher background photon rate, and this can create issues with the telemetry bands and cause the telemetry bands to not include the surface. Even if the actual earth surface is included in the telemetry band, the clouds can affect the travel times and create inaccurate readings \parencite{atl03knownissues}.

During processing from L0 to L1, if NASA reference DEM is not within the telemetry bands, no photons will be classified as signal. Therefore, if the entire granule is effected by this issue, there will be no sea surface found and therefore the entire granule will be filtered out. This can cause a significant loss of data but it is an issue inherent to nearly any remote sensing based approach. One possible way to mitigate this would be to combine the ICESat-2 bathymetry data with synthetic aperture radar (SAR) remote sensing data. SAR remote sensing data can penetrate clouds becaues it uses radiometry outside of the visible spectrum. It cannot directly penetrate even clear water, but SAR data can be used for kinematic satellite-derived bathymetry. The bathymetry estimates from SAR-based kinematic SDB could be incorporated into the Kalman filtering step, and the ICESat-2 data could also be used for validation of the 

In situations where the photons are able to pass through clouds, the changes to travel time through the clouds can affect the accuracy. This could potentially be something that is hard to detect and affect the accuracy of the bathymetric points if any are found.

Additionally, the presence of some clouds along a single granule can cause the loss of data that might otherwise be valid.  

\subsubsection{Multiple Telemetry Bands}

If the signal detection on board the satellite cannot determine where the primary surface is located, it will open another telemetry band to try to collect more signal. This can create other areas of photons that are significantly above or below the surface. The effect of this are shown in \ref{fig:multiple_tel_bands}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/multiple_telemetry_from_known_issues.png}
    \caption{Example of Multiple telemetry from \cite{atl03knownissues}}
    \label{fig:multiple_tel_bands}
\end{figure}

As with data affected by clouds, photons that are a certain distance away from the known reference surface are marked not classified as being signal photons. This has a limited effect because often bathymetric signal points are marked as noise by the default ATL03 classification algorithm used by NASA. 

Most granules that are affected by this issue have photon locations that are significantly above or below the geoid. Therefore, granules affected by this issue area almost always filtered out by the filtering step that removes photons that are significantly above or below the geoid. Those remaining are often well-distributed in the nearshore zone, so they do not increase density in one vertical location enough to create a false positive or affect the existing result. 

\subsubsection{Apparent Multiple Surface Returns and Specular Returns}

Apparent multiple surface returns and specular returns are a similar phenomenon and are both caused by a saturation of the photon detection circitry. 

These could potentially be a huge issue in very still water, and likely \emph{is} negatively affecting results. They artificially increase the density just below the surface so it could easily be misidentified as signal. They occur 2.3 or 4.2 m below the primary surface return.

Figure \ref{fig:multiple-surf-returns} is an example from iceland \pdfcomment{'data_end_utc': '2019-01-14T14:30:20.721564Z', 'atlas_beam_type': 'strong', 'groundtrack_id': 'gt3l'}

\pdfcomment{temporary figure, open in python to add title}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/temp_specular_returns.png}
    \caption{An example of specular returns in a transect with bathymetry signal from Greenland. Bathymetry signal can be seen from 1000-3000m, and the specular returns can be seen from about 2700 to 3800m. From 1 jan 2019, beam gt3l, RGT 262.}
    \label{fig:multiple-surf-returns}
\end{figure}

While these can potentially create a false density that could bias the KDE signal finding function, segments that potentially are affected by these two problems can be flagged based on a parameter that is available as part of the ATL03 data product. 

\subsubsection{TEP Misclassified as Signal}

In manual inspections, no transects have been found to exhibit this problem. However, if it were present this could present two problems to the signal finding algorithm. The first is that it could confound the KDE signal finding by creating more along the TEP. If TEP photons are correctly classified as TEP photons, then they are exluded from all the signal findin. 

The second and likely most significant issue is that the sea surface location is based on the signal classification. If there are incorrectly classified sea surface photons, the algorithm might not exclude the strong signal that is present around the sea surface. For the KDE to work accurately it needs to exlude the stronger signal of the sea surface. 

\subsubsection{Errors Following DMUs}

\pdfcomment{double check on the need for this}
I believe that I have seen this error before, maybe need to check the major activities log to find granules to toss out


\subsection{Water Clarity Requirements}

Most of the world's coasts are sandy and are typically turbid most or all of the year. This presents a significant restriction on spaceborne bathymetry. Of the test sites that were evaluated as part of this project, only sites with exceptionally clear water were able to produce significant bathymetric signal within ICESat-2. Therefore it was evaluated mostly at tropical test sites with a carbonate sedimentary environments.  

\subsection{Temporal limits}

The satellite repeat time for a certain track is every 91 days, and the data is available starting in 2018. Due to off-pointing over most land areas there is often greater than 91 days between passes of the \emph{exact same} reference ground track. Since coasts are among the most dynamic morphological environments, there might be substantial changes between repeat passes. The proposed method assumes that the all the bathymetry data is contemporaneous. This is a inherent limitation to the method, since it cannot account for morphological changes that might occur during the 4-year time window in which the data was collected. Another practical limitation is that not all satellite passes can capture and subsurface data due to issues with the atmospheric conditions, wave heights, instrument conditions, etc. 

\section{Limitations of Kriging Interpolation}

The selected universal kriging interpolator provides both an estimate of the interpolated seafloor surface and the uncertainty of the estimate.

\subsection{Computational Expense}

The largest practical limitation to the bayesian updating method is the requirement to interpolate the data. kriging interpolaters are ideal because the are robust to outlier and provide an uncertainty estimate as well as a depth estimate. The downside of the kriging interpolator is both the comptation expense, and the requirement that points are not too close together. If points are to close to one another, the kriging matrix is not soluble, and the algorithm has a complexity of $\mathcal{O}(n^3)$ where $n$ is the number of points. This means that there is a relatively strict practical limitation to the number of points that can be used as input to the interpolator. The upper limit with a laptop with 32GB RAM was found to be approximately 2000 points without exceeding the available memory. One way to deal with this is to use a tiling strategy, and repeat the process using tiles which contain 2000 bathymetric points or less. This would need to be done adaptively since the bathymetric points are not evenly distributed. Each tile would take longer to solve, but it would at least be feasible using a consumer grade computer. It is also possible that the kriging algorithm could be implemented using a graphics card which might give significant performance gains.

\subsection{Heterogenous Spatial Distribution of Input Data}

The input data is typically along transects whose orientation varies depending on the coastline, and there are often gaps in these transects where the bathymetric signal is weaker, as shown in figure \ref{fig:distribution-of-bathy-points-in-space} The gaps of weaker signal can be caused by areas that are too deep to be optically clear to the lidar, or due to instrument/atmospheric issues discussed in \ref{add}. 

\pdfcomment{replace with a map that has a color scale bar for the bathymetry, includes maybe a subset map and a map collar}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/temp_spatial_dist.png}
    \caption{Spatial distribution of bathymetry points in St. Croix north end, and the true bathymetry}
    \label{fig:distribution-of-bathy-points-in-space}
\end{figure}

\subsection{Kriging Parameter Estimation}


\section{Kalman Updating}


\subsection{Estimation of in-situ standard deviation for GEBCO}


\pdfcomment{Potential section in main body: take all sites with good data, compare the GEBCO RMSE, then compare GEBCO error vs the TID categories}
Could do this based on the TID grid, so far not attempted and just based the gebco uncertainty based on how far 

\subsection{Introducing spurious shoals/shallows}
The proposed method does reliably decrease the RMSE in all case study sites where validation data was available. RMSE as an error metric is biased towards significant outliers, so significant decreases can be interpreted to mean that overall, some of the largest magnitude errors were removed from the GEBCO data. However, it is possible that artifacts introduced by  