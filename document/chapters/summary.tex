\chapter*{Summary}

Bathymetric data is valuable because it is essential for nearly every aspect of coastal management and monitoring but is difficult to obtain, especially in the nearshore zone. Traditional techniques like acoustic sounding provide reasonably high-accuracy data, but survey campaigns are expensive and the survey vessels cannot operate in the shallowest areas of the nearshore zone. Airborne lidar surveying provides high resolution, high accuracy data in the shallowest waters, but the associated survey campaigns are extremely expensive and are very sensitive to environmental conditions. Because of the limitations of conventional survey techniques in the nearshore zone, there is much interest in ways of estimating bathymetry using only spaceborne remote sensing data. Spaceborne remote sensing data is advantageous because of wide global coverage and because the data is often publicly available.
\vskip 0.1in

Traditionally, there are two main techniques for extracting bathymetric estimates from satellite data. One approach, called optical satellite derived bathymetry, estimates the bathymetry based on the relationship between the attenuation of different parts of the visible light spectrum in the water column. These techniques require in-situ data to calibrate, and require the site to have water that is clear down to the seabed. The second approach is to use satellite imagery to estimate the wave field in a region, and then back-calculate the bathymetry based on the evolution of the wave field in space. This is called the wave-kinematic approach. This approach provides less accurate and lower resolution bathymetry estimates than optical SDB, and the deepest depth that can be estimated is limited by the maximum wavelength at the time of the satellite data acquisition. The advantage of wave-kinematic approaches is that the deepest estimation depth is not limited by water clarity.
\vskip 0.1in

NASA's ICESat-2 satellite is a lidar satellite that provides high-accuracy point elevations along linear transects all over the world. It was originally intended to study the elevation of ice in the polar regions, but researchers have discovered that it can sometimes also capture the elevation of the seabed in coasts with very clear water. This has led to a number of papers that extract bathymetric point data from ICESat-2 and use it in combination with optical SDB methods to produce a bathymetric estimate without requiring any in-situ data.  
\vskip 0.1in

Most of the prior studies that extract bathymetric point estimates from ICESat-2 look at a limited number of satellite passes (3--10), and either use manual extraction of the seabed signal based on the researcher's judgement, or use other semi-automated techniques that estimate the seabed based on the density of photons in the vertical direction that are manually checked before use. The goals of this project are to find ways to automate the signal extraction from ICESat-2 data so that it can be implemented at a larger scale (up to hundreds of transects), to evaluate how these sparse point measurements can be interpolated into a continuous gridded product, and to evaluate if this interpolated data can be combined with existing global datasets to add additional value by increasing the resolution and decreasing the error. 
\vskip 0.1in

The methodology proposed is split into 3 major parts. First, the data processing and signal extraction from the satellite data: For a given area of interest, the ICESat-2 data is downloaded and processed to extract only photons that are located in the nearshore zone. It is then corrected for the effect of light refraction in the water column. Then, points that are likely to be sea surface signal are extracted based on the density of photon returns in the vertical direction along a moving window of adjacent photons. The second part of the method is to interpolate these points into a continuous bathymetry surface via universal kriging, a geostatistical approach that takes into account the elevation at each point and the distance between them to produce a gridded output of the estimated interpolated surface and the confidence in the interpolation. The third step is to combine the interpolated surface with a prior estimate, in this case the global bathymetry dataset GEBCO, via a Kalman filter. The Kalman filter is a bayesian approach that takes into account the new estimate and the uncertainty. Areas of the interpolated data with high certainty (i.e., nearer to ICESat-2 point measurements) will have a larger impact on the prior estimate, whereas areas with a lower certainty will have a correspondingly lower magnitude of impact on the prior depth estimate.
\vskip 0.1in

To test the feasibility of the kriging and Kalman updating approach, a synthetic experiment is created using a one of validation data sets. Random point samples from the validation data are taken, and these point samples are used as input to the kriging and kalman updating process. For the study site evaluated here, it is found that by kriging these input points and then combining them with GEBCO data, the error of the combined product is lower than either the error of GEBCO or the kriging surface. The Kriging + Kalman updating approach is also validated using a subset of the JarKus dataset, a series of point measurements of bathymetry surveyed annually along the entire Dutch coast. These data points were used as input to the kriging process, and the output of the kriging process was combined with the prior estimate of the bathymetry (i.e, GEBCO) using the Kalman filter step. The combination of Kriging and Bayesian combination with existing data improves the estimate than either dataset alone, showing that the Bayesian combination approach can add value compared to existing data.
\vskip 0.1in

The entire processing chain is then tested at 4 test sites. The proposed signal extraction technique produced bathymetric point estimates with an RMSE between 0.54 m and 9.53 m at the various sites. The sites with the largest magnitude errors are due to errors in the photon filtering approach, which could be corrected by improving the land mask data. If the subsurface filtering errors are removed, the highest RMSE at a site is 2.42 m. After extracting the bathymetry points and applying the kriging and kalman updating techniques to produce the final output, a gridded product at higher resolution, some sites show a reduction in RMSE of up to 34\% compared to GEBCO. However, a few sites also show an increase in RMSE. This is due primarily to uneven and anisotropic distributions of ICESat-2 point bathymetry estimates at some sites; kriging depends on estimating trends in data, and if the points are not well-distributed in the site it results in a lower quality of the universal kriging interpolation surface.
\vskip 0.1in
To implement this approach at a global scale, the computational efficiency of the processing chain must be maximized. One way to increase the efficiency is to apply the KDE filtering step only to ICESat-2 transects that are the most likely to contain good quality bathymetric signal. Therefore, several ways of pre-selecting the most likely transects to provide good bathymetry signal are investigated. Because high water clarity is required for the lidar signal reach and be returned by the sea bed, the estimated Secchi depth along the ICESat-2 transect was found and compared to the results. No clear relationship between the Secchi depth and the RMSE of the lidar data along a transect was found. Several other transect metadata variables were also checked for their relationship with the transect RMSE; the mean fraction of fully saturated photons, the density of photons along the transect, and the percent of ocean surface photons classified as 'high-confidence' in the default photon classification provided by NASA. However, none of the variables were found to provide a clear way to filter out high-error transects while retaining the low-error transects.
\vskip 0.1in

The proposed processing chain has the potential to increase the horizontal resolution of GEBCO data from 500 m to 50 m while also decreasing the vertical RMS error by up to 35\% without requiring any manually-surveyed data. This result was achieved using the exact same input parameters to the processing chain for every study site; it is likely that the parameter choice could be further optimized to produce better results. The computational bottlenecks in the approach are the KDE signal finding step and the Kriging step. Currently, the approach is practical to implement on consumer computers with a study site of up to 500~$\text{km}^2$, but finding ways to pre-selection of the best transects could allow for significant efficiency gains in the KDE signal finding process. The approach is also limited by the need for exceptionally clear water to extract lidar bathymetric signal. Due to these characteristics, the processing chain proposed here could provide the basis for a global coral reef bathymetry data set.
