\chapter{Discussion - OLD}

How could the accuracy of the method maybe be improved?
\begin{itemize}
    \item scale KDE function by distance 
    \item improve refraction function by including the effect of the local water surface slope
    \item incorporate the a priori confidence (i.e. KDE value strength) into the kriging estimate (well, this might have only a very marginal effect? most of the error probably comes from places where there is no icesat data at all) 
\end{itemize}

what are some weaknesses of the method?

\begin{itemize}
    \item Sometimes shallow bathymetry points are missed due to culling points too close to the sea surface
    \item GEBCO nearshore accuracy is even lower outside the global north due to lack of multibeam data as an input 
\end{itemize}

\pdfcomment{address using lidar photon returns for kinematic bathmetry and the practical implementation}
\pdfcomment{mention use of icesat to improve estimates of global flooding}

\section{Limitations of ICESat-2 ATL03 data}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/multiple_telemetry_from_known_issues.png}
    \caption{Example of Multiple telemetry from \cite{atl03knownissues}}
    \label{fig:multiple_tel_bands}
\end{figure}

As with data affected by clouds, photons that are a certain distance away from the known reference surface are marked not classified as being signal photons. This has a limited effect because often bathymetric signal points are marked as noise by the default ATL03 classification algorithm used by NASA. 

Most granules that are affected by this issue have photon locations that are significantly above or below the geoid. Therefore, granules affected by this issue area almost always filtered out by the filtering step that removes photons that are significantly above or below the geoid. Those remaining are often well-distributed in the nearshore zone, so they do not increase density in one vertical location enough to create a false positive or affect the existing result. 

\subsubsection{Apparent Multiple Surface Returns and Specular Returns}

Apparent multiple surface returns and specular returns are a similar phenomenon and are both caused by a saturation of the photon sensor. These could potentially be a source of error in very still water, and likely \emph{is} negatively affecting results in some sites. They artificially increase the density just below the surface so it could easily be misidentified as signal. They occur 2.3 or 4.2 m below the primary surface return. \pdfcomment{is there a spike in detected depths at 2.3 or 4.2m?}

Figure \ref{fig:multiple-surf-returns} is an example from iceland \pdfcomment{data end utc: 2019 01 14T14:30:20.721564Z atlas beam type: strong groundtrack id: gt3l}

\pdfcomment{temporary figure, open in python to add title}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/temp_specular_returns.png}
    \caption{An example of specular returns in a transect with bathymetry signal from Greenland. Bathymetry signal can be seen from 1000-3000m, and the specular returns can be seen from about 2700 to 3800m. From 1 jan 2019, beam gt3l, RGT 262.}
    \label{fig:multiple-surf-returns}
\end{figure}

While these can potentially create a false density that could bias the KDE signal finding function, segments that potentially are affected by these two problems can be flagged based on a parameter that is available as part of the ATL03 data product. 

\subsubsection{TEP Misclassified as Signal}

In manual inspections, no transects have been found to exhibit this problem. However, if it were present this could present two problems to the signal finding algorithm. The first is that it could confound the KDE signal finding by creating more along the TEP. If TEP photons are correctly classified as TEP photons, then they are exluded from all the signal finding. 

The second and likely most significant issue is that the sea surface location is based on the signal classification. If there are incorrectly classified sea surface photons, the algorithm might not exclude the strong signal that is present around the sea surface. For the KDE to work accurately it needs to exclude the stronger signal of the sea surface. 

\subsection{Water clarity requirements}

Most of the world's coasts are sandy and are typically turbid most or all of the year. This presents a significant restriction on spaceborne bathymetry that requires penetration of the water surface. Of the test sites that were evaluated as part of this project, only sites with exceptionally clear water were able to produce significant bathymetric signal within ICESat-2. Therefore it was evaluated mostly at tropical test sites with a carbonate sedimentary environments.  

\subsection{Temporal limits}

The satellite repeat time for a certain track is every 91 days, and the data is available starting in 2018. Due to off-pointing over most land areas there is often greater than 91 days between passes of the \emph{exact same} reference ground track. Since coasts are among the most dynamic morphological environments, there might be substantial changes between repeat passes. The proposed method assumes that the all the bathymetry data is contemporaneous. This is a inherent limitation to the method, since it cannot account for morphological changes that might occur during the 4-year time window in which the data was collected. Another practical limitation is that not all satellite passes can capture and subsurface data due to issues with the atmospheric conditions, wave heights, instrument conditions, etc. 

\section{Limitations of Kriging Interpolation}

The selected universal kriging interpolator provides both an estimate of the interpolated seafloor surface and the uncertainty of the estimate.

\subsection{Computational Expense}

The largest practical limitation to the bayesian updating method is the requirement to interpolate the data. kriging interpolaters are ideal because the are robust to outlier and provide an uncertainty estimate as well as a depth estimate. The downside of the kriging interpolator is both the comptation expense, and the requirement that points are not too close together. If points are to close to one another, the kriging matrix is not soluble, and the algorithm has a complexity of $\mathcal{O}(n^3)$ where $n$ is the number of points. This means that there is a relatively strict practical limitation to the number of points that can be used as input to the interpolator. The upper limit with a laptop with 32GB RAM was found to be approximately 2000 points without exceeding the available memory. One way to deal with this is to use a tiling strategy, and repeat the process using tiles which contain 2000 bathymetric points or less. This would need to be done adaptively since the bathymetric points are not evenly distributed. Each tile would take longer to solve, but it would at least be feasible using a consumer-grade computer. It is also possible that the kriging algorithm could be implemented using a graphics card which might give significant performance gains.

\subsection{Heterogenous spatial distribution of ICESat-2 photons}

Due to the orbit of ICESat-2, the ATL03 data is available along transects $\pm \ang{6}$ relative to north. Because of this pattern, even if there was perfect bathmetry data along every single ICESat-2 transect within the study area, the spatial distribution of the points would be anisotropic. Given that there are gaps in these transects where the bathymetric signal is weaker, the spatial distribution of the data can very significantly by site. These gaps along transects can be seen in figure \ref{fig:distribution-of-bathy-points-in-space}. The gaps of weaker signal can be caused by areas that are too deep to be optically clear to the lidar, or due to instrument/atmospheric issues (\ref{sec:discussion-photon-issues}). 

This irregular distribution means that in some places far away from bathymetric signal, the upscaled estimate might not be any better than the original, so any accuracy gains are likely unevenly distributed across the site. 

\pdfcomment{discuss results of random sampling tests}

\pdfcomment{replace with a map that has a color scale bar for the bathymetry, includes maybe a subset map and a map collar}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/temp_spatial_dist.png}
    \caption{Spatial distribution of bathymetry points in St. Croix north end, and the true bathymetry}
    \label{fig:distribution-of-bathy-points-in-space}
\end{figure}


\section{Kalman Updating}

\subsection{Estimation of in-situ standard deviation for GEBCO}


\pdfcomment{Potential section in main body: take all sites with good data, compare the GEBCO RMSE, then compare GEBCO error vs the TID categories}



\subsection{Introducing spurious shoals/shallows}

The proposed method does reliably decrease the RMSE in all case study sites where it was applied. RMS error is sensitive to significant outliers, so large decreases in RMSE can be interpreted to mean that the largest magnitude errors were removed from the GEBCO data. However, it is possible that artifacts introduced by the kriging interpolator could introduce some high or low points that do not reflect physical features, especially in areas of the test site that are far from ICESat-2 bathymetry points.